{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Zeroth-order natural gradient attack on MNIST\r\n",
    "\r\n",
    "We implement a minimum working example of using natural gradient, computed using black-box samples and randomized inversion, to craft adversarial attacks on MNIST.\r\n",
    "\r\n",
    "@author: (Anonymous)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from prettytable import PrettyTable\r\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This block of code fetches the data, and defines a function that\r\n",
    "# splits the data into test/train, and into batches.\r\n",
    "# Note that this function will only download the data once. Subsequent \r\n",
    "# calls will load the data from the hard drive\r\n",
    "\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import torchvision.datasets as datasets\r\n",
    "import torch \r\n",
    "\r\n",
    "def MNIST_Loaders(train_batch_size, test_batch_size=None):\r\n",
    "    if test_batch_size is None:\r\n",
    "        test_batch_size = train_batch_size\r\n",
    "    \r\n",
    "    normalize = transforms.Normalize((0.,), (1.,))\r\n",
    "#     normalize = transforms.Normalize((0.1307,), (0.3081,))\r\n",
    "    Clean = transforms.Compose([transforms.ToTensor(), normalize])\r\n",
    "   \r\n",
    "    #!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\r\n",
    "    #!tar -zxvf MNIST.tar.gz\r\n",
    "    \r\n",
    "    train_data = datasets.MNIST('./', train=True,\r\n",
    "                                   download=True, transform=Clean)\r\n",
    "    test_data = datasets.MNIST('./', train=False,\r\n",
    "                                  download=True, transform=Clean)\r\n",
    "    \r\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\r\n",
    "                    batch_size=train_batch_size)\r\n",
    "    \r\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\r\n",
    "                    batch_size=test_batch_size)\r\n",
    "    \r\n",
    "    return train_loader, test_loader\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# This block of code sets up the network. We'll use the LeNet-5\r\n",
    "# architecture.\r\n",
    "\r\n",
    "input_size = [28,28]\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Net,self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6,\r\n",
    "                               kernel_size=5 , stride=1)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\r\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,\r\n",
    "                              kernel_size=5, stride=1) \r\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120,\r\n",
    "                              kernel_size=4, stride=1)\r\n",
    "        self.fc_1 = nn.Linear(in_features=120,out_features=84)  \r\n",
    "        self.fc_2 = nn.Linear(in_features=84, out_features=10)\r\n",
    "            \r\n",
    "    def forward(self,u):\r\n",
    "        u = self.conv1(u)  # apply first convolutional layer\r\n",
    "        u = self.relu(u)   # apply ReLU activation\r\n",
    "        u = self.pool(u)   # apply max-pooling\r\n",
    "        u = self.conv2(u)  # apply second convolutional layer\r\n",
    "        u = self.relu(u)   # Apply ReLU activation\r\n",
    "        u = self.pool(u)\r\n",
    "        u = self.conv3(u)  # Apply third and final convolutional layer\r\n",
    "        u = torch.flatten(u, 1)\r\n",
    "        u = self.fc_1(u)\r\n",
    "        u = self.relu(u)\r\n",
    "        u = self.fc_2(u)\r\n",
    "        u = self.relu(u)\r\n",
    "        y = F.log_softmax(u, dim=1)\r\n",
    "        return y\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Some useful functions to keep tabs on the training, courtesy of Samy Wu Fung\r\n",
    "\r\n",
    "def get_stats(net, test_loader):\r\n",
    "        test_loss=0\r\n",
    "        correct=0\r\n",
    "        with torch.no_grad():\r\n",
    "            for d_test, labels in test_loader:\r\n",
    "                batch_size = d_test.shape[0]\r\n",
    "                y = net(d_test)  # apply the network to the test data\r\n",
    "                test_loss += batch_size*F.nll_loss(y, labels).item() # sum up batch loss\r\n",
    "                \r\n",
    "                pred = y.argmax(dim=1,keepdim=True)\r\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\r\n",
    "                \r\n",
    "        test_loss /= len(test_loader.dataset)\r\n",
    "        test_acc = 100.*correct/len(test_loader.dataset)\r\n",
    "        \r\n",
    "        return test_loss, test_acc, correct\r\n",
    "                \r\n",
    "        \r\n",
    "def model_params(net):\r\n",
    "    table = PrettyTable([\"Network Component\", \"# Parameters\"])\r\n",
    "    num_params = 0\r\n",
    "    for name, parameter in net.named_parameters():\r\n",
    "        if not parameter.requires_grad:\r\n",
    "            continue\r\n",
    "        table.add_row([name, parameter.numel()])\r\n",
    "        num_params += parameter.numel()\r\n",
    "    table.add_row(['Total', num_params])\r\n",
    "    return table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# The training function\r\n",
    "\r\n",
    "def train_net(net, num_epochs, train_loader, test_loader, optimizer,\r\n",
    "              checkpt_path):\r\n",
    "    \r\n",
    "    loss_ave = 0.0\r\n",
    "    train_acc = 0.0\r\n",
    "    best_test_acc = 0.0\r\n",
    "    \r\n",
    "    test_loss_hist = []\r\n",
    "    test_acc_hist = []\r\n",
    "    \r\n",
    "    print(net)\r\n",
    "    print(model_params(net))\r\n",
    "    print('\\nTraining Network')\r\n",
    "    \r\n",
    "    # initialize a learning rate scheduler\r\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        tot = len(train_loader)\r\n",
    "        \r\n",
    "        for idx, (data, labels) in enumerate(train_loader):\r\n",
    "            batch_size = data.shape[0]\r\n",
    "            \r\n",
    "            optimizer.zero_grad()\r\n",
    "            \r\n",
    "            # forward and backward then take a step of optimizer\r\n",
    "            y = net(data)\r\n",
    "            loss = F.nll_loss(y, labels)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            \r\n",
    "        # Output some training stats\r\n",
    "        if (epoch+1) % 1 == 0:\r\n",
    "            test_loss, test_acc, correct = get_stats(net, test_loader)\r\n",
    "            print('Number of epochs= {:03d} Test loss = {:.3f} and Test accuracy = {:.3f}'.format(epoch+1, test_loss, test_acc))\r\n",
    "            \r\n",
    "        # Save weights every ten epochs\r\n",
    "        if (epoch +1) % 10 == 0 and test_acc > best_test_acc:\r\n",
    "            best_test_acc = test_acc\r\n",
    "            state = {\r\n",
    "                'test_loss_hist': test_loss_hist,\r\n",
    "                'test_acc_hist': test_acc_hist,\r\n",
    "                'net_state_dict': net.state_dict(),\r\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\r\n",
    "            }\r\n",
    "            file_name = checkpt_path + 'MNIST_weights.pth'\r\n",
    "            torch.save(state, file_name)\r\n",
    "            print('Model weights saved to ' + file_name)\r\n",
    "            \r\n",
    "        # advance the learning rate schedule\r\n",
    "        scheduler.step()\r\n",
    "    return net\r\n",
    "        \r\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Initialize model and prepare for training\r\n",
    "load_weights = False\r\n",
    "\r\n",
    "model = Net()\r\n",
    "max_epochs=20 # train for max_epochs full passes over the data\r\n",
    "learning_rate=1.0\r\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\r\n",
    "checkpt_path = './models/'  # Best practice is to periodically save, or checkpoint the weights\r\n",
    "batch_size = 128\r\n",
    "save_dir = './'\r\n",
    "\r\n",
    "if load_weights:\r\n",
    "    state = torch.load('modelsMNIST_weights.pth')\r\n",
    "    model.load_state_dict(state['net_state_dict'])\r\n",
    "\r\n",
    "print(model_params(model))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+--------------+\n",
      "| Network Component | # Parameters |\n",
      "+-------------------+--------------+\n",
      "|    conv1.weight   |     150      |\n",
      "|     conv1.bias    |      6       |\n",
      "|    conv2.weight   |     2400     |\n",
      "|     conv2.bias    |      16      |\n",
      "|    conv3.weight   |    30720     |\n",
      "|     conv3.bias    |     120      |\n",
      "|    fc_1.weight    |    10080     |\n",
      "|     fc_1.bias     |      84      |\n",
      "|    fc_2.weight    |     840      |\n",
      "|     fc_2.bias     |      10      |\n",
      "|       Total       |    44426     |\n",
      "+-------------------+--------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_loader, test_loader = MNIST_Loaders(batch_size)\r\n",
    "model = train_net(model, max_epochs, train_loader, test_loader,\r\n",
    "                 optimizer, checkpt_path)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc_1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc_2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "+-------------------+--------------+\n",
      "| Network Component | # Parameters |\n",
      "+-------------------+--------------+\n",
      "|    conv1.weight   |     150      |\n",
      "|     conv1.bias    |      6       |\n",
      "|    conv2.weight   |     2400     |\n",
      "|     conv2.bias    |      16      |\n",
      "|    conv3.weight   |    30720     |\n",
      "|     conv3.bias    |     120      |\n",
      "|    fc_1.weight    |    10080     |\n",
      "|     fc_1.bias     |      84      |\n",
      "|    fc_2.weight    |     840      |\n",
      "|     fc_2.bias     |      10      |\n",
      "|       Total       |    44426     |\n",
      "+-------------------+--------------+\n",
      "\n",
      "Training Network\n",
      "Number of epochs= 001 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 002 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 003 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 004 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 005 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 006 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 007 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 008 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 009 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 010 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Model weights saved to ./models/MNIST_weights.pth\n",
      "Number of epochs= 011 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 012 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 013 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 014 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 015 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 016 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 017 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 018 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 019 Test loss = 0.033 and Test accuracy = 98.860\n",
      "Number of epochs= 020 Test loss = 0.033 and Test accuracy = 98.860\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}